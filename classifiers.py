# -*- coding: utf-8 -*-
"""Classifiers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e4qfwY3hA6WLq0kDcwAe8TQoRsjhMLcg
"""

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
from matplotlib import colormaps
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

class Classifiers():
    def __init__(self,data):
        '''
        TODO: Write code to convert the given pandas dataframe into training and testing data
        # all the data should be nxd arrays where n is the number of samples and d is the dimension of the data
        # all the labels should be nx1 vectors with binary labels in each entry
        '''
        X = df.drop('label', axis=1)
        y = df['label']

        self.training_data, self.testing_data, self.training_labels, self.testing_labels = train_test_split(X, y, test_size=0.4, random_state=42)
        self.outputs = [] #for recording the outputs

    def test_clf(self, clf, classifier_name='', param_grid=None, plot_decision_boundary=True):
        # TODO: Fit the classifier and extrach the best score, training score and parameters
        gs = GridSearchCV(clf, param_grid, cv=5)
        gs.fit(self.training_data, self.training_labels)
        best_params = gs.best_params_
        best_clf = gs.best_estimator_
        print(f"Best parameters for {classifier_name}: {best_params}.")

        # Evaluate the classifier
        training_score = best_clf.score(self.training_data, self.training_labels)
        testing_score = best_clf.score(self.testing_data, self.testing_labels)

        #records the name and scores to outputs
        self.outputs.append(f'{classifier_name}, {training_score}, {testing_score}')

        # Plot the decision boundary if required
        if plot_decision_boundary:
            self.plot(self.testing_data.to_numpy(), self.testing_labels.to_numpy(), model=best_clf, classifier_name=classifier_name)
        print(f'Training score: {training_score}, Testing Score: {testing_score}')
        print()
        return training_score, testing_score


    def classifyNearestNeighbors(self):
        # TODO: Write code to run a Nearest Neighbors classifier
        knn = KNeighborsClassifier()
        param_grid = {
            'n_neighbors': range(2, 20),
            'leaf_size': [5,10,15,20,25,30]
        }
        self.test_clf(knn, classifier_name='K-Nearest Neighbors', param_grid=param_grid)
    pass

    def classifyLogisticRegression(self):
        # TODO: Write code to run a Logistic Regression classifier
        logReg = LogisticRegression(random_state=16)
        param_grid = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}
        self.test_clf(logReg, classifier_name="Logistic Regression", param_grid=param_grid)
    pass

    def classifyDecisionTree(self):
        # TODO: Write code to run a Logistic Regression classifier
        decisionTree = DecisionTreeClassifier()
        param_grid = {
            'max_depth': range(1,51),
            'min_samples_split': range(2,11)
        }
        self.test_clf(decisionTree, classifier_name="Decision Tree", param_grid=param_grid)
        pass

    def classifyRandomForest(self):
        # TODO: Write code to run a Random Forest classifier
        randomForest = RandomForestClassifier()
        param_grid = {
            'max_depth':[1,2,3,4,5],
            'min_samples_split': range(2,11)
        }
        self.test_clf(randomForest, classifier_name="Random Forest", param_grid=param_grid)
        pass

    def classifyAdaBoost(self):
        # TODO: Write code to run a AdaBoost classifier
        adaBoost = AdaBoostClassifier()
        param_grid = {'n_estimators': [10,20,30,40,50,60,70]}
        self.test_clf(adaBoost, classifier_name="AdaBoost", param_grid=param_grid)
        pass

    def plot(self, X, Y, model,classifier_name = ''):
        X1 = X[:, 0]
        X2 = X[:, 1]

        X1_min, X1_max = min(X1) - 0.5, max(X1) + 0.5
        X2_min, X2_max = min(X2) - 0.5, max(X2) + 0.5

        X1_inc = (X1_max - X1_min) / 200.
        X2_inc = (X2_max - X2_min) / 200.

        X1_surf = np.arange(X1_min, X1_max, X1_inc)
        X2_surf = np.arange(X2_min, X2_max, X2_inc)
        X1_surf, X2_surf = np.meshgrid(X1_surf, X2_surf)

        L_surf = model.predict(np.c_[X1_surf.ravel(), X2_surf.ravel()])
        L_surf = L_surf.reshape(X1_surf.shape)

        plt.title(classifier_name)
        plt.contourf(X1_surf, X2_surf, L_surf, cmap = plt.cm.coolwarm, zorder = 1)
        plt.scatter(X1, X2, s = 38, c = Y)

        plt.margins(0.0)
        # uncomment the following line to save images
        # plt.savefig(f'{classifier_name}.png')
        plt.show()


if __name__ == "__main__":
    df = pd.read_csv('input.csv')

    #creating scatter plot
    plt.figure(figsize=(8, 6))
    plt.scatter(df['A'], df['B'], c=df['label'], cmap='coolwarm')

    models = Classifiers(df)
    print('Classifying with NN...')
    models.classifyNearestNeighbors()
    print('Classifying with Logistic Regression...')
    models.classifyLogisticRegression()
    print('Classifying with Decision Tree...')
    models.classifyDecisionTree()
    print('Classifying with Random Forest...')
    models.classifyRandomForest()
    print('Classifying with AdaBoost...')
    models.classifyAdaBoost()

    with open("output.csv", "w") as f:
        print('Name, Best Training Score, Testing Score',file=f)
        for line in models.outputs:
            print(line, file=f)